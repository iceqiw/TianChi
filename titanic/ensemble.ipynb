{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "excited-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4) (45, 4) (105,) (45,)\n"
     ]
    }
   ],
   "source": [
    "## 1. 简单堆叠3折CV分类\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''载入数据'''\n",
    "X,y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "'''不采取分层抽样时的数据集分割'''\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "'''打印各个数据集的形状'''\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "helpful-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-fold cross validation:\n",
      "\n",
      "Accuracy: 0.91 (+/- 0.01) [KNN]\n",
      "Accuracy: 0.95 (+/- 0.01) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.02) [Naive Bayes]\n",
      "Accuracy: 0.90 (+/- 0.00) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "## 1. 简单堆叠3折CV分类\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Starting from v0.16.0, StackingCVRegressor supports\n",
    "# `random_state` to get deterministic result.\n",
    "sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],  # 第一层分类器\n",
    "                            meta_classifier=lr,   # 第二层分类器\n",
    "                            random_state=RANDOM_SEED)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], ['KNN', 'Random Forest', 'Naive Bayes','StackingClassifier']):\n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "veterinary-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4) (45, 4) (105,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss2 = StandardScaler()\n",
    "ss2.fit(X_train)\n",
    "train_data_X_sd = ss2.transform(X_train)\n",
    "test_data_X_sd = ss2.transform(X_test)\n",
    "X = train_data_X_sd\n",
    "X_test = test_data_X_sd\n",
    "y=y_train\n",
    "print(X.shape,X_test.shape,y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acceptable-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "'''模型融合中使用到的各个单模型'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    " \n",
    "clfs = [LogisticRegression(C=0.1,max_iter=100),\n",
    "        xgb.XGBClassifier(max_depth=6,n_estimators=100,num_round = 5),\n",
    "        RandomForestClassifier(n_estimators=100,max_depth=6,oob_score=True),\n",
    "        GradientBoostingClassifier(learning_rate=0.3,max_depth=6,n_estimators=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "olympic-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4) (45, 4) (105,)\n",
      "the 0 training start ...\n",
      "the 1 training start ...\n",
      "the 2 training start ...\n",
      "the 0 training start ...\n",
      "[21:07:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:07:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 1 training start ...\n",
      "[21:07:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:07:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 2 training start ...\n",
      "[21:07:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:07:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 0 training start ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiwei/dev/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1 training start ...\n",
      "the 2 training start ...\n",
      "the 0 training start ...\n",
      "the 1 training start ...\n",
      "the 2 training start ...\n",
      "[0.18208763 0.18208763 0.18208763 0.18208763 0.18208763 0.18208763\n",
      " 0.1820876  0.18208762 0.18208763 0.18208763 0.18208763 0.18208763\n",
      " 0.18208763 0.18208761 0.18208763 0.18208763 0.18208763 0.18208763\n",
      " 0.18208763 0.18208762 0.18208763 0.18208763 0.18208428 0.18208763\n",
      " 0.18208763 0.18208763 0.18208762 0.18208763 0.18208755 0.18208763\n",
      " 0.18208762 0.18208762 0.18208754 0.18208763 0.18208763 0.18208763\n",
      " 0.18208763 0.18208763 0.18208763 0.18208763 0.18208763 0.18208763\n",
      " 0.18208763 0.18208763 0.18208763]\n"
     ]
    }
   ],
   "source": [
    "# 创建n_folds\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 创建零矩阵\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_test.shape[0], len(clfs)))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=2019)\n",
    "print(X.shape,X_test.shape,y.shape)\n",
    "for j, clf in enumerate(clfs):\n",
    "    dataset_blend_test_j = np.zeros((X_test.shape[0], 3))\n",
    "    for n_fold,(tr_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "        print(f'the {n_fold} training start ...')\n",
    "        tr_x, tr_y, val_x, val_y = X[tr_idx], y[tr_idx], X[val_idx], y[val_idx]\n",
    "        clf.fit(tr_x, tr_y)\n",
    "        y_submission = clf.predict_proba(val_x)[:,1]\n",
    "        dataset_blend_train[val_idx, j] = y_submission\n",
    "        dataset_blend_test_j[:, n_fold] = clf.predict_proba(X_test)[:, 1]\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "#     print(dataset_blend_test[1])\n",
    "\n",
    "clf2 = LogisticRegression(C=0.1,max_iter=100)\n",
    "clf2.fit(dataset_blend_train, y)\n",
    "y_submission = clf2.predict_proba(dataset_blend_test)[:, 1]\n",
    "print(y_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coated-opposition",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e8146f8d8b2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_data_X_sd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data_X_sd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss2 = StandardScaler()\n",
    "ss2.fit(X_train)\n",
    "train_data_X_sd = ss2.transform(X_train)\n",
    "test_data_X_sd = ss2.transform(X_test)\n",
    " \n",
    "\n",
    " \n",
    "##划分train数据集,调用代码,把数据集名字转成和代码一样\n",
    "X = train_data_X_sd\n",
    "X_predict = test_data_X_sd\n",
    "y = train_data_Y\n",
    " \n",
    "'''模型融合中使用到的各个单模型'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    " \n",
    "clfs = [LogisticRegression(C=0.1,max_iter=100),\n",
    "        xgb.XGBClassifier(max_depth=6,n_estimators=100,num_round = 5),\n",
    "        RandomForestClassifier(n_estimators=100,max_depth=6,oob_score=True),\n",
    "        GradientBoostingClassifier(learning_rate=0.3,max_depth=6,n_estimators=100)]\n",
    " \n",
    "# 创建n_folds\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "n_folds = 5\n",
    "skf = list(StratifiedKFold(y, n_folds))\n",
    " \n",
    "# 创建零矩阵\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    " \n",
    "# 建立模型\n",
    "for j, clf in enumerate(clfs):\n",
    "    '''依次训练各个单模型'''\n",
    "    # print(j, clf)\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "        # print(\"Fold\", i)\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n",
    "    '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    " \n",
    "# 用建立第二层模型\n",
    "clf2 = LogisticRegression(C=0.1,max_iter=100)\n",
    "clf2.fit(dataset_blend_train, y)\n",
    "y_submission = clf2.predict_proba(dataset_blend_test)[:, 1]\n",
    " \n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test[\"Survived\"] = clf2.predict(dataset_blend_test)\n",
    "test[['PassengerId','Survived']].set_index('PassengerId').to_csv('stack3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
